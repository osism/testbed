---
heat_template_version: 2018-08-31

###########
parameters:

  image:
    type: string
    constraints:
      - custom_constraint: glance.image
    default: Ubuntu 18.04
{% if number_of_volumes|default(3)|int > 0 and number_of_nodes|default(3)|int > 0 %}
  volume_size_storage:
    type: number
    default: 10
{% endif -%}
{% if number_of_nodes|default(3)|int > 0 %}
  flavor_node:
    type: string
    constraints:
      - custom_constraint: nova.flavor
    default: 4C-16GB-40GB
{% endif %}
  flavor_manager:
    type: string
    constraints:
      - custom_constraint: nova.flavor
    default: 2C-4GB-20GB

  availability_zone:
    type: string
    default: south-2

  volume_availability_zone:
    type: string
    default: south-2

  network_availability_zone:
    type: string
    default: south-2

  public:
    type: string
    constraints:
      - custom_constraint: neutron.network
    default: external

  deploy_infrastructure:
    type: boolean
    default: false

  deploy_openstack:
    type: boolean
    default: false

  deploy_ceph:
    type: boolean
    default: false

  run_refstack:
    type: boolean
    default: false

  configuration_version:
    type: string
    default: master

  ceph_version:
    type: string
    default: nautilus
    constraints:
      - allowed_values:
          - luminous
          - nautilus
          - octopus

  openstack_version:
    type: string
    default: train
    constraints:
      - allowed_values:
          - rocky
          - stein
          - train

##########
resources:

  #########
  # Generic
  #########

  key:
    type: OS::Nova::KeyPair
    properties:
      name: testbed
      save_private_key: true

  manager_wait_handle:
{%- if number_of_nodes|default(3)|int > 0 %}
    depends_on: node_wait_condition
{%- endif %}
    type: OS::Heat::WaitConditionHandle

  manager_wait_condition:
    type: OS::Heat::WaitCondition
    properties:
      handle: {get_resource: manager_wait_handle}
      count: 1
      timeout: 7200
{% if number_of_nodes|default(3)|int > 0 %}
  node_wait_handle:
    type: OS::Heat::WaitConditionHandle

  node_wait_condition:
    type: OS::Heat::WaitCondition
    properties:
      handle: {get_resource: node_wait_handle}
      count: {{ number_of_nodes|default(3)|int }}
      timeout: 1800
{% endif %}
  manager_boot_config:
    type: OS::Heat::CloudConfig
    properties:
      cloud_config:
        package_update: true
        package_upgrade: false
        packages:
          - ifupdown
        write_files:
          - content: {get_attr: [key, public_key]}
            path: /home/ubuntu/.ssh/id_rsa.pub
            permissions: 0600
          - content: {get_attr: [key, private_key]}
            path: /home/ubuntu/.ssh/id_rsa
            permissions: 0600
          - content:
              str_replace:
                params:
                  deploy_ceph: {get_param: deploy_ceph}
                  deploy_infrastructure: {get_param: deploy_infrastructure}
                  deploy_openstack: {get_param: deploy_openstack}
                  configuration_version: {get_param: configuration_version}
                  ceph_version: {get_param: ceph_version}
                  openstack_version: {get_param: openstack_version}
                  wc_notify: {get_attr: ['manager_wait_handle', 'curl_cli']}
                template: |
                  #!/usr/bin/env bash

                  chown -R ubuntu:ubuntu /home/ubuntu/.ssh

                  add-apt-repository --yes ppa:ansible/ansible
                  apt-get install --yes ansible

                  ansible-galaxy install git+https://github.com/osism/ansible-chrony
                  ansible-galaxy install git+https://github.com/osism/ansible-common
                  ansible-galaxy install git+https://github.com/osism/ansible-operator
                  ansible-galaxy install git+https://github.com/osism/ansible-repository
                  ansible-galaxy install git+https://github.com/osism/ansible-resolvconf

                  curl https://raw.githubusercontent.com/osism/testbed/configuration_version/playbooks/node.yml > /root/node.yml
                  ansible-playbook -i localhost, /root/node.yml

                  cp /home/ubuntu/.ssh/id_rsa /home/dragon/.ssh/id_rsa
                  cp /home/ubuntu/.ssh/id_rsa.pub /home/dragon/.ssh/id_rsa.pub
                  chown -R dragon:dragon /home/dragon/.ssh

                  sudo -iu dragon ansible-galaxy install git+https://github.com/osism/ansible-configuration
                  sudo -iu dragon ansible-galaxy install git+https://github.com/osism/ansible-docker
                  sudo -iu dragon ansible-galaxy install git+https://github.com/osism/ansible-manager

                  curl https://raw.githubusercontent.com/osism/testbed/configuration_version/playbooks/manager-part-1.yml | sudo -iu dragon tee /home/dragon/manager-part-1.yml
                  curl https://raw.githubusercontent.com/osism/testbed/configuration_version/playbooks/manager-part-2.yml | sudo -iu dragon tee /home/dragon/manager-part-2.yml
                  curl https://raw.githubusercontent.com/osism/testbed/configuration_version/playbooks/manager-part-3.yml | sudo -iu dragon tee /home/dragon/manager-part-3.yml

                  sudo -iu dragon ansible-playbook -i testbed-manager.osism.local, /home/dragon/manager-part-1.yml -e configuration_git_version=configuration_version
                  sudo -iu dragon sh -c 'cd /opt/configuration; ./scripts/set-ceph-version.sh ceph_version'
                  sudo -iu dragon sh -c 'cd /opt/configuration; ./scripts/set-openstack-version.sh openstack_version'

                  sudo -iu dragon ansible-playbook -i testbed-manager.osism.local, /home/dragon/manager-part-2.yml
                  sudo -iu dragon ansible-playbook -i testbed-manager.osism.local, /home/dragon/manager-part-3.yml

                  sudo -iu dragon docker cp /home/dragon/.ssh/id_rsa.pub manager_osism-ansible_1:/share/id_rsa.pub

                  rm /home/ubuntu/.ssh/id_rsa*

                  # NOTE(berendt): wait for ARA
                  until [[ "$(/usr/bin/docker inspect -f '{{ '{{' }}.State.Health.Status{{ '}}' }}' manager_ara-server_1)" == "healthy" ]]; do
                      sleep 1;
                  done;

                  curl https://raw.githubusercontent.com/osism/testbed/configuration_version/playbooks/cleanup.yml > /root/cleanup.yml
                  ansible-playbook -i localhost, /root/cleanup.yml
                  update-alternatives --install /usr/bin/python python /usr/bin/python3 1

                  # NOTE(berendt): sudo -E does not work here because sudo -i is needed

                  sudo -iu dragon sh -c 'INTERACTIVE=false osism-run custom cronjobs'
                  sudo -iu dragon sh -c 'INTERACTIVE=false osism-run custom facts'

                  # deploy proxy services
                  sudo -iu dragon sh -c '/opt/configuration/scripts/deploy_proxy_services.sh'

                  # prepare all systems
                  if [[ {{ number_of_nodes|default(3)|int }} -gt 0 ]]; then
                      sudo -iu dragon sh -c 'INTERACTIVE=false osism-generic bootstrap'
                      sudo -iu dragon sh -c 'INTERACTIVE=false osism-generic operator'

                      # NOTE: Restart the manager services to update the /etc/hosts file
                      sudo -iu dragon sh -c 'docker-compose -f /opt/manager/docker-compose.yml restart'

                      # NOTE(berendt): wait for ARA
                      until [[ "$(/usr/bin/docker inspect -f '{{ '{{' }}.State.Health.Status{{ '}}' }}' manager_ara-server_1)" == "healthy" ]]; do
                          sleep 1;
                      done;

                      # deploy helper services
                      sudo -iu dragon sh -c '/opt/configuration/scripts/deploy_helper_services.sh'
                  else
                      sudo -iu dragon sh -c 'INTERACTIVE=false osism-generic operator -l manager'
                  fi

                  # copy network configuration
                  sudo -iu dragon sh -c 'INTERACTIVE=false osism-generic network'

                  # deploy infrastructure services
                  if [[ "deploy_infrastructure" == "True" ]]; then
                      sudo -iu dragon sh -c '/opt/configuration/scripts/deploy_infrastructure_services.sh'
                  fi

                  # deploy ceph services
                  if [[ "deploy_ceph" == "True" ]]; then
                      sudo -iu dragon sh -c '/opt/configuration/scripts/deploy_ceph_services.sh'
                  fi

                  # deploy openstack services
                  if [[ "deploy_openstack" == "True" ]]; then
                      if [[ "deploy_infrastructure" != "True" ]]; then
                          echo "infrastructure services are necessary for the deployment of OpenStack"
                      else
                          sudo -iu dragon sh -c '/opt/configuration/scripts/deploy_openstack_services_basic.sh'

                          if [[ "run_refstack" == "True" ]]; then
                              sudo -iu dragon sh -c '/opt/configuration/contrib/refstack/refstack.sh'
                          fi
                      fi
                  fi

                  wc_notify --data-binary '{"status": "SUCCESS"}'
            path: /root/run.sh
            permissions: 0700
        runcmd:
          - "echo 'network: {config: disabled}' > /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg"
          - "rm -f /etc/network/interfaces.d/50-cloud-init.cfg"
          - "mv /etc/netplan/50-cloud-init.yaml /etc/netplan/50-cloud-init.yaml.unused"
          - "/root/run.sh"
        final_message: "The system is finally up, after $UPTIME seconds"
        power_state:
          mode: reboot
          condition: true
{% if number_of_nodes|default(3)|int > 0 %}
  node_boot_config:
    type: OS::Heat::CloudConfig
    depends_on: node_wait_handle
    properties:
      cloud_config:
        package_update: true
        package_upgrade: false
        packages:
          - ifupdown
        write_files:
          - content: {get_attr: [key, public_key]}
            path: /home/ubuntu/.ssh/id_rsa.pub
            permissions: 0600
          - content: {get_attr: [key, private_key]}
            path: /home/ubuntu/.ssh/id_rsa
            permissions: 0600
          - content:
              str_replace:
                params:
                  configuration_version: {get_param: configuration_version}
                  wc_notify: {get_attr: ['node_wait_handle', 'curl_cli']}
                template: |
                  #!/usr/bin/env bash

                  wc_notify --data-binary '{"status": "SUCCESS"}'

                  chown -R ubuntu:ubuntu /home/ubuntu/.ssh

                  add-apt-repository --yes ppa:ansible/ansible
                  apt-get install --yes ansible

                  ansible-galaxy install git+https://github.com/osism/ansible-chrony
                  ansible-galaxy install git+https://github.com/osism/ansible-common
                  ansible-galaxy install git+https://github.com/osism/ansible-operator
                  ansible-galaxy install git+https://github.com/osism/ansible-repository
                  ansible-galaxy install git+https://github.com/osism/ansible-resolvconf

                  curl https://raw.githubusercontent.com/osism/testbed/configuration_version/playbooks/node.yml > /root/node.yml
                  ansible-playbook -i localhost, /root/node.yml

                  curl https://raw.githubusercontent.com/osism/testbed/configuration_version/playbooks/cleanup.yml > /root/cleanup.yml
                  ansible-playbook -i localhost, /root/cleanup.yml
                  update-alternatives --install /usr/bin/python python /usr/bin/python3 1

                  rm /home/ubuntu/.ssh/id_rsa*
            path: /root/run.sh
            permissions: 0700
        runcmd:
          - "echo 'network: {config: disabled}' > /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg"
          - "rm -f /etc/network/interfaces.d/50-cloud-init.cfg"
          - "mv /etc/netplan/50-cloud-init.yaml /etc/netplan/50-cloud-init.yaml.unused"
          - "/root/run.sh"
        final_message: "The system is finally up, after $UPTIME seconds"
{% endif %}
  #################
  # Security groups
  #################

  security_group_manager:
    type: OS::Neutron::SecurityGroup
    properties:
      name: testbed-manager
      rules:
        - remote_ip_prefix: 0.0.0.0/0
          protocol: udp
          port_range_min: 51820
          port_range_max: 51820

  security_group_management:
    type: OS::Neutron::SecurityGroup
    properties:
      name: testbed-management
      rules:
        - remote_ip_prefix: 0.0.0.0/0
          protocol: tcp
          port_range_min: 22
          port_range_max: 22
        - remote_ip_prefix: 0.0.0.0/0
          protocol: icmp

  security_group_internal:
    type: OS::Neutron::SecurityGroup
    properties:
      name: testbed-internal
      rules:
        - remote_ip_prefix: 0.0.0.0/0
          protocol: 112
        - remote_ip_prefix: 0.0.0.0/0
          protocol: tcp
          port_range_min: 1
          port_range_max: 65535
        - remote_ip_prefix: 0.0.0.0/0
          protocol: udp
          port_range_min: 1
          port_range_max: 65535
        - remote_ip_prefix: 0.0.0.0/0
          protocol: icmp
{% if number_of_nodes|default(3)|int > 0 %}
  security_group_storage_frontend:
    type: OS::Neutron::SecurityGroup
    properties:
      name: testbed-storage-frontend
      rules:
        - remote_ip_prefix: 0.0.0.0/0
          protocol: tcp
          port_range_min: 1
          port_range_max: 65535
        - remote_ip_prefix: 0.0.0.0/0
          protocol: udp
          port_range_min: 1
          port_range_max: 65535
        - remote_ip_prefix: 0.0.0.0/0
          protocol: icmp

  security_group_storage_backend:
    type: OS::Neutron::SecurityGroup
    properties:
      name: testbed-storage-backend
      rules:
        - remote_ip_prefix: 0.0.0.0/0
          protocol: tcp
          port_range_min: 1
          port_range_max: 65535
        - remote_ip_prefix: 0.0.0.0/0
          protocol: udp
          port_range_min: 1
          port_range_max: 65535
        - remote_ip_prefix: 0.0.0.0/0
          protocol: icmp

  security_group_external:
    type: OS::Neutron::SecurityGroup
    properties:
      name: testbed-external
      rules:
        - remote_ip_prefix: 0.0.0.0/0
          protocol: tcp
          port_range_min: 1
          port_range_max: 65535
        - remote_ip_prefix: 0.0.0.0/0
          protocol: udp
          port_range_min: 1
          port_range_max: 65535
        - remote_ip_prefix: 0.0.0.0/0
          protocol: icmp
{% endif %}
  ############
  # Networks #
  ############

  net_management:
    type: OS::Neutron::Net
    properties:
      name: testbed-management
      value_specs:
        availability_zone_hints:
          - {get_param: network_availability_zone}

  subnet_management:
    type: OS::Neutron::Subnet
    properties:
      network: {get_resource: net_management}
      cidr: 192.168.40.0/24
      allocation_pools:
        -
          start: 192.168.40.100
          end: 192.168.40.110

  net_internal:
    type: OS::Neutron::Net
    properties:
      name: testbed-internal
      value_specs:
        availability_zone_hints:
          - {get_param: network_availability_zone}

  subnet_internal:
    type: OS::Neutron::Subnet
    properties:
      network: {get_resource: net_internal}
      cidr: 192.168.50.0/24
      gateway_ip: null
      enable_dhcp: false
      allocation_pools:
        -
          start: 192.168.50.100
          end: 192.168.50.110
{% if number_of_nodes|default(3)|int > 0 %}
  net_provider:
    type: OS::Neutron::Net
    properties:
      name: testbed-provider
      value_specs:
        availability_zone_hints:
          - {get_param: network_availability_zone}

  subnet_provider:
    type: OS::Neutron::Subnet
    properties:
      network: {get_resource: net_provider}
      cidr: 192.168.100.0/24
      gateway_ip: null
      enable_dhcp: false
      allocation_pools:
        -
          start: 192.168.100.100
          end: 192.168.100.110

  net_external:
    type: OS::Neutron::Net
    properties:
      name: testbed-external
      value_specs:
        availability_zone_hints:
          - {get_param: network_availability_zone}

  subnet_external:
    type: OS::Neutron::Subnet
    properties:
      network: {get_resource: net_external}
      cidr: 192.168.90.0/24
      gateway_ip: null
      enable_dhcp: false
      allocation_pools:
        -
          start: 192.168.90.100
          end: 192.168.90.110

  vip_port_external:
    type: OS::Neutron::Port
    properties:
      network_id: {get_resource: net_external}
      fixed_ips:
        - ip_address: 192.168.90.200

  vip_port_internal:
    type: OS::Neutron::Port
    properties:
      network_id: {get_resource: net_internal}
      fixed_ips:
        - ip_address: 192.168.50.200

  net_storage_frontend:
    type: OS::Neutron::Net
    properties:
      name: testbed-storage-frontend
      value_specs:
        availability_zone_hints:
          - {get_param: network_availability_zone}

  subnet_storage_frontend:
    type: OS::Neutron::Subnet
    properties:
      network: {get_resource: net_storage_frontend}
      cidr: 192.168.70.0/24
      gateway_ip: null
      enable_dhcp: false
      allocation_pools:
        -
          start: 192.168.70.100
          end: 192.168.70.110

  net_storage_backend:
    type: OS::Neutron::Net
    properties:
      name: testbed-storage-backend
      value_specs:
        availability_zone_hints:
          - {get_param: network_availability_zone}

  subnet_storage_backend:
    type: OS::Neutron::Subnet
    properties:
      network: {get_resource: net_storage_backend}
      cidr: 192.168.80.0/24
      gateway_ip: null
      enable_dhcp: false
      allocation_pools:
        -
          start: 192.168.80.100
          end: 192.168.80.110
{% endif %}
  ##########################
  # Network infrastructure #
  ##########################

  router:
    type: OS::Neutron::Router
    properties:
      external_gateway_info:
        network: {get_param: public}
      value_specs:
        availability_zone_hints:
          - {get_param: network_availability_zone}

  router_interface:
    type: OS::Neutron::RouterInterface
    properties:
      router: {get_resource: router}
      subnet: {get_resource: subnet_management}

  ###########
  # Manager #
  ###########

  manager_floating_ip:
    type: OS::Neutron::FloatingIP
    depends_on: router_interface
    properties:
      floating_network_id: {get_param: public}
      port_id: {get_resource: manager_port_management}

  manager_port_management:
    type: OS::Neutron::Port
    properties:
      network_id: {get_resource: net_management}
      fixed_ips:
        - ip_address: 192.168.40.5
      security_groups:
        - {get_resource: security_group_management}
        - {get_resource: security_group_manager}

  manager_port_internal:
    type: OS::Neutron::Port
    properties:
      network_id: {get_resource: net_internal}
      fixed_ips:
        - ip_address: 192.168.50.5
      allowed_address_pairs:
        - ip_address: 192.168.60.0/24
      security_groups:
        - {get_resource: security_group_internal}
{% if number_of_nodes|default(3)|int > 0 %}
  manager_port_external:
    type: OS::Neutron::Port
    properties:
      network_id: {get_resource: net_external}
      fixed_ips:
        - ip_address: 192.168.90.5
      allowed_address_pairs:
        - ip_address: 192.168.60.0/24
      security_groups:
        - {get_resource: security_group_external}

  manager_port_provider:
    type: OS::Neutron::Port
    properties:
      network_id: {get_resource: net_provider}
      port_security_enabled: false
      fixed_ips:
        - ip_address: 192.168.100.5

  manager_port_storage_frontend:
    type: OS::Neutron::Port
    properties:
      network_id: {get_resource: net_storage_frontend}
      fixed_ips:
        - ip_address: 192.168.70.5
      security_groups:
        - {get_resource: security_group_storage_frontend}
{% endif %}
  manager_server:
    type: OS::Nova::Server
{%- if number_of_nodes|default(3)|int > 0 %}
    depends_on: node_wait_condition
{%- endif %}
    properties:
      name: testbed-manager
      key_name: {get_resource: key}
      image: {get_param: image}
      flavor: {get_param: flavor_manager}
      availability_zone: {get_param: availability_zone}
      user_data_format: SOFTWARE_CONFIG
      user_data: {get_resource: manager_boot_config}
      metadata:
        group: manager
      config_drive: true
      networks:
        - port: {get_resource: manager_port_management}
        - port: {get_resource: manager_port_internal}
{%- if number_of_nodes|default(3)|int > 0 %}
        - port: {get_resource: manager_port_external}
        - port: {get_resource: manager_port_provider}
        - port: {get_resource: manager_port_storage_frontend}
{% endif %}
  #########
  # Nodes #
  #########
{% for n in range(number_of_nodes|default(3)|int) %}
  # node {{ n }}
{% for m in range(number_of_volumes|default(3)|int) %}
  node_{{ n }}_volume_{{ m }}:
    type: OS::Cinder::Volume
    properties:
      name: testbed-node-{{ n }}-volume-{{ m }}
      size: {get_param: volume_size_storage}
      availability_zone: {get_param: volume_availability_zone}

  node_{{ n }}_volume_{{ m }}_attachment:
    type: OS::Cinder::VolumeAttachment
    properties:
      instance_uuid: {get_resource: node_{{ n }}_server}
      volume_id: {get_resource: node_{{ n }}_volume_{{ m }}}
{% endfor %}
  node_{{ n }}_port_management:
    type: OS::Neutron::Port
    properties:
      network_id: {get_resource: net_management}
      fixed_ips:
        - ip_address: 192.168.40.1{{ n }}
      security_groups:
        - {get_resource: security_group_management}

  node_{{ n }}_port_internal:
    type: OS::Neutron::Port
    properties:
      network_id: {get_resource: net_internal}
      fixed_ips:
        - ip_address: 192.168.50.1{{ n }}
      allowed_address_pairs:
        - ip_address: 192.168.50.200/32
      security_groups:
        - {get_resource: security_group_internal}

  node_{{ n }}_port_external:
    type: OS::Neutron::Port
    properties:
      network_id: {get_resource: net_external}
      fixed_ips:
        - ip_address: 192.168.90.1{{ n }}
      allowed_address_pairs:
        - ip_address: 192.168.90.200/32
      security_groups:
        - {get_resource: security_group_external}

  node_{{ n }}_port_provider:
    type: OS::Neutron::Port
    properties:
      network_id: {get_resource: net_provider}
      port_security_enabled: false
      fixed_ips:
        - ip_address: 192.168.100.1{{ n }}

  node_{{ n }}_port_storage_frontend:
    type: OS::Neutron::Port
    properties:
      network_id: {get_resource: net_storage_frontend}
      fixed_ips:
        - ip_address: 192.168.70.1{{ n }}
      security_groups:
        - {get_resource: security_group_storage_frontend}

  node_{{ n }}_port_storage_backend:
    type: OS::Neutron::Port
    properties:
      network_id: {get_resource: net_storage_backend}
      fixed_ips:
        - ip_address: 192.168.80.1{{ n }}
      security_groups:
        - {get_resource: security_group_storage_backend}

  node_{{ n }}_server:
    type: OS::Nova::Server
    properties:
      name: testbed-node-{{ n }}
      key_name: {get_resource: key}
      image: {get_param: image}
      flavor: {get_param: flavor_node}
      availability_zone: {get_param: availability_zone}
      user_data_format: SOFTWARE_CONFIG
      user_data: {get_resource: node_boot_config}
      metadata:
        group: node
      config_drive: true
      networks:
        - port: {get_resource: node_{{ n }}_port_management}
        - port: {get_resource: node_{{ n }}_port_internal}
        - port: {get_resource: node_{{ n }}_port_external}
        - port: {get_resource: node_{{ n }}_port_provider}
        - port: {get_resource: node_{{ n }}_port_storage_frontend}
        - port: {get_resource: node_{{ n }}_port_storage_backend}
{% endfor %}

########
outputs:

  manager_address:
    value: {get_attr: [manager_floating_ip, floating_ip_address]}

  manager_address_management:
    value: {get_attr: [manager_port_management, fixed_ips, 0, ip_address]}

  manager_address_internal:
    value: {get_attr: [manager_port_internal, fixed_ips, 0, ip_address]}

  public_key:
    value: {get_attr: [key, public_key]}

  private_key:
    value: {get_attr: [key, private_key]}
