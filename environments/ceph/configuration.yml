---
ceph_image_version: octopus

##########################
# generic

containerized_deployment: true

generate_fsid: false
fsid: 11111111-1111-1111-1111-111111111111

##########################
# osd

osd_objectstore: bluestore
osd_scenario: lvm

dmcrypt: true

# NOTE: It is common to place more than 1 OSD on flash devices. To simulate
#       upgrades etc. with this approach this is also used here.
osds_per_device: 2

##########################
# network

public_network: 192.168.64.0/20
cluster_network: 192.168.80.0/20

##########################
# pools & keys

# NOTE: default size of 2 because by default there are only 2 nodes
rgw_pool_default_size: 2
rgw_pool_default_pg_num: 8

rgw_zone: default
rgw_create_pools:
  "{{ rgw_zone }}.rgw.buckets.data":
    pg_num: "{{ rgw_pool_default_pg_num }}"
    size: "{{ rgw_pool_default_size }}"
    type: replicated
  "{{ rgw_zone }}.rgw.buckets.index":
    pg_num: "{{ rgw_pool_default_pg_num }}"
    size: "{{ rgw_pool_default_size }}"
    type: replicated
  "{{ rgw_zone }}.rgw.meta":
    pg_num: "{{ rgw_pool_default_pg_num }}"
    size: "{{ rgw_pool_default_size }}"
    type: replicated
  "{{ rgw_zone }}.rgw.log":
    pg_num: "{{ rgw_pool_default_pg_num }}"
    size: "{{ rgw_pool_default_size }}"
    type: replicated
  "{{ rgw_zone }}.rgw.control":
    pg_num: "{{ rgw_pool_default_pg_num }}"
    size: "{{ rgw_pool_default_size }}"
    type: replicated

# NOTE: default size of 2 because by default there are only 2 nodes
cephfs_pool_default_size: 2
cephfs_pool_default_min_size: 0
cephfs_pool_default_pg_num: 16

cephfs_data_pool:
  name: "cephfs_data"
  pg_num: "{{ cephfs_pool_default_pg_num }}"
  pgp_num: "{{ cephfs_pool_default_pg_num }}"
  rule_name: "replicated_rule"
  type: 1
  erasure_profile: ""
  expected_num_objects: ""
  application: "cephfs"
  size: "{{ cephfs_pool_default_size }}"
  min_size: "{{ cephfs_pool_default_min_size }}"

cephfs_metadata_pool:
  name: "cephfs_metadata"
  pg_num: "{{ cephfs_pool_default_pg_num }}"
  pgp_num: "{{ cephfs_pool_default_pg_num }}"
  rule_name: "replicated_rule"
  type: 1
  erasure_profile: ""
  expected_num_objects: ""
  application: "cephfs"
  size: "{{ cephfs_pool_default_size }}"
  min_size: "{{ cephfs_pool_default_min_size }}"

cephfs_pools:
  - "{{ cephfs_data_pool }}"
  - "{{ cephfs_metadata_pool }}"

# NOTE: After the initial deployment of the Ceph Clusters, the following parameter can be
#       set to false. It must only be set to true again when new pools or keys are added.
openstack_config: true

# NOTE: default size of 2 because by default there are only 2 nodes
openstack_pool_default_size: 2
openstack_pool_default_min_size: 0
openstack_pool_default_pg_num: 64

openstack_cinder_backup_pool:
  name: "backups"
  pg_num: "{{ openstack_pool_default_pg_num }}"
  pgp_num: "{{ openstack_pool_default_pg_num }}"
  rule_name: "replicated_rule"
  type: 1
  erasure_profile: ""
  expected_num_objects: ""
  application: "rbd"
  size: "{{ openstack_pool_default_size }}"
  min_size: "{{ openstack_pool_default_min_size }}"
  pg_autoscale_mode: false

openstack_cinder_pool:
  name: "volumes"
  pg_num: "{{ openstack_pool_default_pg_num }}"
  pgp_num: "{{ openstack_pool_default_pg_num }}"
  rule_name: "replicated_rule"
  type: 1
  erasure_profile: ""
  expected_num_objects: ""
  application: "rbd"
  size: "{{ openstack_pool_default_size }}"
  min_size: "{{ openstack_pool_default_min_size }}"
  pg_autoscale_mode: false

openstack_glance_pool:
  name: "images"
  pg_num: "{{ openstack_pool_default_pg_num }}"
  pgp_num: "{{ openstack_pool_default_pg_num }}"
  rule_name: "replicated_rule"
  type: 1
  erasure_profile: ""
  expected_num_objects: ""
  application: "rbd"
  size: "{{ openstack_pool_default_size }}"
  min_size: "{{ openstack_pool_default_min_size }}"
  pg_autoscale_mode: false

openstack_gnocchi_pool:
  name: "metrics"
  pg_num: "{{ openstack_pool_default_pg_num }}"
  pgp_num: "{{ openstack_pool_default_pg_num }}"
  rule_name: "replicated_rule"
  type: 1
  erasure_profile: ""
  expected_num_objects: ""
  application: "rbd"
  size: "{{ openstack_pool_default_size }}"
  min_size: "{{ openstack_pool_default_min_size }}"
  pg_autoscale_mode: false

openstack_nova_pool:
  name: "vms"
  pg_num: "{{ openstack_pool_default_pg_num }}"
  pgp_num: "{{ openstack_pool_default_pg_num }}"
  rule_name: "replicated_rule"
  type: 1
  erasure_profile: ""
  expected_num_objects: ""
  application: "rbd"
  size: "{{ openstack_pool_default_size }}"
  min_size: "{{ openstack_pool_default_min_size }}"
  pg_autoscale_mode: false

openstack_pools:
  - "{{ openstack_cinder_backup_pool }}"
  - "{{ openstack_cinder_pool }}"
  - "{{ openstack_glance_pool }}"
  - "{{ openstack_gnocchi_pool }}"
  - "{{ openstack_nova_pool }}"

openstack_keys:
  - name: client.cinder-backup
    caps:
      mon: "profile rbd"
      osd: "profile rbd pool={{ openstack_cinder_backup_pool.name }}"
    mode: "0600"
  - name: client.cinder
    caps:
      mon: "profile rbd"
      osd: "profile rbd pool={{ openstack_cinder_pool.name }}, profile rbd pool={{ openstack_nova_pool.name }}, profile rbd pool={{ openstack_glance_pool.name }}"
    mode: "0600"
  - name: client.glance
    caps:
      mon: "profile rbd"
      osd: "profile rbd pool={{ openstack_cinder_pool.name }}, profile rbd pool={{ openstack_glance_pool.name }}"
    mode: "0600"
  - name: client.gnocchi
    caps:
      mon: "profile rbd"
      osd: "profile rbd pool={{ openstack_gnocchi_pool.name }}"
    mode: "0600"
  - name: client.nova
    caps:
      mon: "profile rbd"
      osd: "profile rbd pool={{ openstack_glance_pool.name }}, profile rbd pool={{ openstack_nova_pool.name }}, profile rbd pool={{ openstack_cinder_pool.name }}, profile rbd pool={{ openstack_cinder_backup_pool.name }}"
    mode: "0600"

##########################
# manager

ceph_mgr_modules:
  - balancer
  - dashboard
  - prometheus
  - status

##########################
# rgw

radosgw_frontend_port: 8081

##########################
# custom

ceph_conf_overrides:
  global:
    # NOTE: default size of 2 because by default there are only 2 nodes
    osd pool default size: 2
    osd pool default min size: 0

  mon:
    mon allow pool delete: true

  "client.rgw.{{ hostvars[inventory_hostname]['ansible_hostname'] }}.rgw0":
    "rgw content length compat": "true"
    "rgw enable apis": "swift, s3"
    "rgw keystone accepted roles": "Member, _member_, admin"
    "rgw keystone accepted admin roles": "admin"
    "rgw keystone admin domain": "default"
    "rgw keystone admin password": "hF6NWPG4rWTpK00oANEcRAiKbwbEcKFHHYYskar2"
    "rgw keystone admin project": "service"
    "rgw keystone admin tenant": "service"
    "rgw keystone admin user": "swift"
    "rgw keystone api version": "3"
    "rgw keystone revocation interval": "900"
    "rgw keystone url": "http://api-int.osism.test:35357"
    "rgw keystone implicit tenants": "true"
    "rgw s3 auth use keystone": "true"
    "rgw swift account in url": "true"
    "rgw swift versioning enabled": "true"
